{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "Untitled.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dantae74/machine-learning/blob/main/codeit/06-02-logistic-regression-gradient-descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfgQSbgOoe-2"
      },
      "source": [
        "# 로지스틱 회귀 - 경사하강"
      ],
      "id": "WfgQSbgOoe-2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RK3QTbWDofS7"
      },
      "source": [
        "import numpy as np"
      ],
      "id": "RK3QTbWDofS7",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30dxHmEPog-b"
      },
      "source": [
        "def sigmoid(x):\n",
        "    \"\"\"시그모이드 함수\"\"\"\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "    \n",
        "    \n",
        "def prediction(X, theta):\n",
        "    \"\"\"로지스틱 회귀 가정 함수\"\"\"\n",
        "    # 지난 과제에서 작성한 코드를 갖고 오세요\n",
        "    return sigmoid(X @ theta)\n",
        "    \n",
        "\n",
        "def gradient_descent(X, theta, y, iterations, alpha):\n",
        "    \"\"\"로지스틱 회귀 경사 하강 알고리즘\"\"\"\n",
        "    m = len(X)  # 입력 변수 개수 저장\n",
        "\n",
        "    for _ in range(iterations):\n",
        "        # 코드를 쓰세요\n",
        "        error = prediction(X, theta) - y\n",
        "        theta = theta - alpha * 1/m * (X.T @ error)\n",
        "            \n",
        "    return theta\n",
        "    \n",
        "    \n"
      ],
      "id": "30dxHmEPog-b",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0p2uSW32S6o",
        "outputId": "4f8254f6-e3d9-4e77-8f8f-18a9331ba215",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 입력 변수\n",
        "hours_studied = np.array([0.2, 0.3, 0.7, 1, 1.3, 1.8, 2, 2.1, 2.2, 3, 4, 4.2, 4, 4.7, 5.0, 5.9])  # 공부 시간 (단위: 100시간)\n",
        "gpa_rank = np.array([0.9, 0.95, 0.8, 0.82, 0.7, 0.6, 0.55, 0.67, 0.4, 0.3, 0.2, 0.2, 0.15, 0.18, 0.15, 0.05]) # 학년 내신 (백분률)\n",
        "number_of_tries = np.array([1, 2, 2, 2, 4, 2, 2, 2, 3, 3, 3, 3, 2, 4, 1, 2])  # 시험 응시 횟수\n",
        "\n",
        "# 목표 변수\n",
        "passed = np.array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1])  # 시험 통과 여부 (0: 탈락, 1:통과)\n",
        "\n",
        "# 설계 행렬 X 정의\n",
        "X = np.array([\n",
        "    np.ones(16),\n",
        "    hours_studied,\n",
        "    gpa_rank,\n",
        "    number_of_tries\n",
        "]).T\n",
        "\n",
        "# 입력 변수 y 정의\n",
        "y = passed\n",
        "\n",
        "theta = [0, 0, 0, 0]  # 파라미터 초기값 설정\n",
        "theta = gradient_descent(X, theta, y, 300, 0.1)  # 경사 하강법을 사용해서 최적의 파라미터를 찾는다\n",
        "theta"
      ],
      "id": "j0p2uSW32S6o",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1.35280508,  1.61640725, -1.83666046, -0.60286277])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJzpqxb-2XPV"
      },
      "source": [
        ""
      ],
      "id": "hJzpqxb-2XPV",
      "execution_count": null,
      "outputs": []
    }
  ]
}