{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "04-titanic-machine-learning-from-disaster.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dantae74/machine-learning/blob/main/kaggle/04-titanic-machine-learning-from-disaster.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfgQSbgOoe-2"
      },
      "source": [
        "### kaggle 에서 가져왔습니다.\n",
        "\n",
        "# Titanic - Machine Learning from Diaster"
      ],
      "id": "WfgQSbgOoe-2"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import sklearn\n",
        "import xgboost as xgb\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "pO0JtS3YSEeq"
      },
      "id": "pO0JtS3YSEeq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# kaggle에서 titanic 데이터 셋 가져오기"
      ],
      "metadata": {
        "id": "WUsyfLUDQEWL"
      },
      "id": "WUsyfLUDQEWL"
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install kaggle"
      ],
      "metadata": {
        "id": "P1FQF2c_QOqu"
      },
      "id": "P1FQF2c_QOqu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "GSaaZQEOQOcv"
      },
      "id": "GSaaZQEOQOcv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cp kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "7pUVFxwMQOPo"
      },
      "id": "7pUVFxwMQOPo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "HhgoZ4kIRin6"
      },
      "id": "HhgoZ4kIRin6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle competitions download -c titanic"
      ],
      "metadata": {
        "id": "nzlGcSjsRlPX"
      },
      "id": "nzlGcSjsRlPX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.offline as py\n",
        "py.init_notebook_mode(connected=True)\n",
        "import plotly.graph_objs as go\n",
        "import plotly.tools as tls"
      ],
      "metadata": {
        "id": "IbetDQTQSPJz"
      },
      "id": "IbetDQTQSPJz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "7WxO8zHHKVBS"
      },
      "id": "7WxO8zHHKVBS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Going to use these 5 base models for the stacking\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import KFold"
      ],
      "metadata": {
        "id": "RMrubStgKfxp"
      },
      "id": "RMrubStgKfxp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Exploration, Engineering and Cleaning"
      ],
      "metadata": {
        "id": "d0gdejsuOgOf"
      },
      "id": "d0gdejsuOgOf"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in the train and test datasets\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')"
      ],
      "metadata": {
        "id": "gMulSOw4MIgB"
      },
      "id": "gMulSOw4MIgB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store our passenger ID for easy access\n",
        "PassengerId = test['PassengerId']"
      ],
      "metadata": {
        "id": "cyjGlNxFSC7m"
      },
      "id": "cyjGlNxFSC7m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head(3)"
      ],
      "metadata": {
        "id": "6u5UbQw_uZeq"
      },
      "id": "6u5UbQw_uZeq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_data = [train, test]"
      ],
      "metadata": {
        "id": "mXS2T3LBurhx"
      },
      "id": "mXS2T3LBurhx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Some features of my own that I have added in\n",
        "# Gives the length of the name\n",
        "train['Name_length'] = train['Name'].apply(len)\n",
        "test['Name_length'] = test['Name'].apply(len)"
      ],
      "metadata": {
        "id": "dArjWwSUvTlu"
      },
      "id": "dArjWwSUvTlu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature that tells whether a passenger had a cabin on the Titanic\n",
        "train['Has_Cabin'] = train['Cabin'].apply(lambda x: 0 if pd.isnull(x) else 1)\n",
        "test['Has_Cabin'] = test['Cabin'].apply(lambda x: 0 if pd.isnull(x) else 1)"
      ],
      "metadata": {
        "id": "04xJrKv-vVw4"
      },
      "id": "04xJrKv-vVw4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature engineering steps taken from Sina\n",
        "# Create new feature FamilySize as a combination of SibSp and Parch\n",
        "for dataset in full_data:\n",
        "    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1"
      ],
      "metadata": {
        "id": "eLo_Jjup0TLs"
      },
      "id": "eLo_Jjup0TLs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new feature IsAlone from FamilySize\n",
        "for dataset in full_data:\n",
        "    dataset['IsAlone'] = 0\n",
        "    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1"
      ],
      "metadata": {
        "id": "A_TECsCEkQCG"
      },
      "id": "A_TECsCEkQCG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove all NULLS in the Embarked column\n",
        "for dataset in full_data:\n",
        "  dataset['Embarked'] = dataset['Embarked'].fillna('S')"
      ],
      "metadata": {
        "id": "Xa295IvtkSi9"
      },
      "id": "Xa295IvtkSi9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove all NULLS in the Fare column and create a new feature CategoricalFare\n",
        "for dataset in full_data:\n",
        "  dataset['Fare'] = dataset['Fare'].fillna(dataset['Fare'].median())\n",
        "\n",
        "train['CategoricalFare'] = pd.qcut(train['Fare'], 4)"
      ],
      "metadata": {
        "id": "GKYvQYZ2kYOF"
      },
      "id": "GKYvQYZ2kYOF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a New feature CategoricalAge\n",
        "for dataset in full_data:\n",
        "  age_avg = dataset['Age'].mean()\n",
        "  age_std = dataset['Age'].std()\n",
        "  age_null_count = dataset['Age'].isnull().sum()\n",
        "  age_null_random_list = np.random.randint(age_avg-age_std, age_avg+age_std, size=age_null_count, dtype=int)\n",
        "  dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\n",
        "  dataset['Age'] = dataset['Age'].astype(int)\n",
        "  \n",
        "train['CategoricalAge'] = pd.qcut(train['Age'], 5)"
      ],
      "metadata": {
        "id": "u2wMd1j_kl4n"
      },
      "id": "u2wMd1j_kl4n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to extract titles from passenger names\n",
        "def get_title(name):\n",
        "  title_search = re.search('([a-zA-Z]+)\\.', name)\n",
        "  if title_search:\n",
        "    return title_search.group(1)\n",
        "  return \"\""
      ],
      "metadata": {
        "id": "iUADqctqkaav"
      },
      "id": "iUADqctqkaav",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new feature Title, containing the titles of passenger names\n",
        "for dataset in full_data:\n",
        "  dataset['Title'] = dataset['Name'].apply(get_title)"
      ],
      "metadata": {
        "id": "lFw0RpfQ8pwc"
      },
      "id": "lFw0RpfQ8pwc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group all non-common titles into one single grouping \"Rare\"\n",
        "for dataset in full_data:\n",
        "    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
        "\n",
        "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n",
        "    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n",
        "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')"
      ],
      "metadata": {
        "id": "7h6FlbNg8ptS"
      },
      "id": "7h6FlbNg8ptS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for dataset in full_data:\n",
        "    # Mapping Sex\n",
        "    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n",
        "    \n",
        "    # Mapping titles\n",
        "    title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n",
        "    dataset['Title'] = dataset['Title'].map(title_mapping)\n",
        "    dataset['Title'] = dataset['Title'].fillna(0)\n",
        "    \n",
        "    # Mapping Embarked\n",
        "    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n",
        "    \n",
        "    # Mapping Fare\n",
        "    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] \t\t\t\t\t\t        = 0\n",
        "    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n",
        "    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n",
        "    dataset.loc[ dataset['Fare'] > 31, 'Fare'] \t\t\t\t\t\t\t        = 3\n",
        "    dataset['Fare'] = dataset['Fare'].astype(int)\n",
        "    \n",
        "    # Mapping Age\n",
        "    dataset.loc[ dataset['Age'] <= 16, 'Age'] \t\t\t\t\t       = 0\n",
        "    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n",
        "    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n",
        "    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n",
        "    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4 ;"
      ],
      "metadata": {
        "id": "ZUqdEAom_kGh"
      },
      "id": "ZUqdEAom_kGh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature selection\n",
        "drop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\n",
        "train = train.drop(drop_elements, axis = 1)\n",
        "train = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\n",
        "test  = test.drop(drop_elements, axis = 1)"
      ],
      "metadata": {
        "id": "Ywk_xEdJANRY"
      },
      "id": "Ywk_xEdJANRY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization"
      ],
      "metadata": {
        "id": "r0lVBZWmBruB"
      },
      "id": "r0lVBZWmBruB"
    },
    {
      "cell_type": "code",
      "source": [
        "train.head()"
      ],
      "metadata": {
        "id": "TcueTO118pqk"
      },
      "id": "TcueTO118pqk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pearson Correlation Heatmap"
      ],
      "metadata": {
        "id": "CzQ12FfSCNIi"
      },
      "id": "CzQ12FfSCNIi"
    },
    {
      "cell_type": "code",
      "source": [
        "colormap = plt.cm.RdBu\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
        "sns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, \n",
        "            square=True, cmap=colormap, linecolor='white', annot=True)"
      ],
      "metadata": {
        "id": "qerzw6QFCR-l"
      },
      "id": "qerzw6QFCR-l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pairplots"
      ],
      "metadata": {
        "id": "UVHEWU8Oo0Aj"
      },
      "id": "UVHEWU8Oo0Aj"
    },
    {
      "cell_type": "code",
      "source": [
        "g = sns.pairplot(train[[u'Survived', u'Pclass', u'Sex', u'Age', u'Parch', u'Fare', u'Embarked',\n",
        "       u'FamilySize', u'Title']], hue='Survived', palette = 'seismic',size=1.2,diag_kind = 'kde',\n",
        "       diag_kws=dict(shade=True),plot_kws=dict(s=10) )\n",
        "g.set(xticklabels=[])"
      ],
      "metadata": {
        "id": "uCuanysECn1v"
      },
      "id": "uCuanysECn1v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ensembling & Stacking models"
      ],
      "metadata": {
        "id": "NvfAPg-RrbXa"
      },
      "id": "NvfAPg-RrbXa"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helpers via Python Classes"
      ],
      "metadata": {
        "id": "qRJvEP0B0_OX"
      },
      "id": "qRJvEP0B0_OX"
    },
    {
      "cell_type": "code",
      "source": [
        "# Some useful parameters which will come in handy later on\n",
        "ntrain = train.shape[0]\n",
        "ntest = test.shape[0]\n",
        "SEED = 0 # for reproducibility\n",
        "NFOLDS = 5 # set folds for out-of-fold prediction\n",
        "kf = KFold(n_splits= NFOLDS, shuffle=True, random_state=SEED)\n",
        "\n",
        "# Class to extend the Sklearn classifier\n",
        "class SklearnHelper(object):\n",
        "    def __init__(self, clf, seed=0, params=None):\n",
        "        params['random_state'] = seed\n",
        "        self.clf = clf(**params)\n",
        "\n",
        "    def train(self, x_train, y_train):\n",
        "        self.clf.fit(x_train, y_train)\n",
        "\n",
        "    def predict(self, x):\n",
        "        return self.clf.predict(x)\n",
        "    \n",
        "    def fit(self,x,y):\n",
        "        return self.clf.fit(x,y)\n",
        "    \n",
        "    def feature_importances(self,x,y):\n",
        "        print(self.clf.fit(x,y).feature_importances_)\n",
        "    \n",
        "# Class to extend XGboost classifer"
      ],
      "metadata": {
        "id": "7CPOV4yQouxr"
      },
      "id": "7CPOV4yQouxr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_oof(clf, x_train, y_train, x_test):\n",
        "    oof_train = np.zeros((ntrain,))\n",
        "    oof_test = np.zeros((ntest,))\n",
        "    oof_test_skf = np.empty((NFOLDS, ntest))\n",
        "\n",
        "    for i, (train_index, test_index) in enumerate(kf.split(train)):\n",
        "        x_tr = x_train[train_index]\n",
        "        y_tr = y_train[train_index]\n",
        "        x_te = x_train[test_index]\n",
        "\n",
        "        clf.train(x_tr, y_tr)\n",
        "\n",
        "        oof_train[test_index] = clf.predict(x_te)\n",
        "        oof_test_skf[i, :] = clf.predict(x_test)\n",
        "\n",
        "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
        "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)"
      ],
      "metadata": {
        "id": "9IDOt9y7piRM"
      },
      "id": "9IDOt9y7piRM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating our Base First-Level Models\n",
        "\n",
        "1. Random Forest classifier\n",
        "2. Extra Trees classifier\n",
        "3. AdaBoost classifer\n",
        "4. Gradient Boosting classifer\n",
        "5. Support Vector Machine"
      ],
      "metadata": {
        "id": "HMhWZ8TR35RR"
      },
      "id": "HMhWZ8TR35RR"
    },
    {
      "cell_type": "code",
      "source": [
        "# Put in our parameters for said classifiers\n",
        "# Random Forest parameters\n",
        "rf_params = {\n",
        "    'n_jobs': -1,\n",
        "    'n_estimators': 500,\n",
        "     'warm_start': True, \n",
        "     #'max_features': 0.2,\n",
        "    'max_depth': 6,\n",
        "    'min_samples_leaf': 2,\n",
        "    'max_features' : 'sqrt',\n",
        "    'verbose': 0\n",
        "}\n",
        "\n",
        "# Extra Trees Parameters\n",
        "et_params = {\n",
        "    'n_jobs': -1,\n",
        "    'n_estimators':500,\n",
        "    #'max_features': 0.5,\n",
        "    'max_depth': 8,\n",
        "    'min_samples_leaf': 2,\n",
        "    'verbose': 0\n",
        "}\n",
        "\n",
        "# AdaBoost parameters\n",
        "ada_params = {\n",
        "    'n_estimators': 500,\n",
        "    'learning_rate' : 0.75\n",
        "}\n",
        "\n",
        "# Gradient Boosting parameters\n",
        "gb_params = {\n",
        "    'n_estimators': 500,\n",
        "     #'max_features': 0.2,\n",
        "    'max_depth': 5,\n",
        "    'min_samples_leaf': 2,\n",
        "    'verbose': 0\n",
        "}\n",
        "\n",
        "# Support Vector Classifier parameters \n",
        "svc_params = {\n",
        "    'kernel' : 'linear',\n",
        "    'C' : 0.025\n",
        "}"
      ],
      "metadata": {
        "id": "I3biddJ5COuM"
      },
      "id": "I3biddJ5COuM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create 5 objects that represent our 4 models\n",
        "rf = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params)\n",
        "et = SklearnHelper(clf=ExtraTreesClassifier, seed=SEED, params=et_params)\n",
        "ada = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params)\n",
        "gb = SklearnHelper(clf=GradientBoostingClassifier, seed=SEED, params=gb_params)\n",
        "svc = SklearnHelper(clf=SVC, seed=SEED, params=svc_params)"
      ],
      "metadata": {
        "id": "QV65SsxDCMI1"
      },
      "id": "QV65SsxDCMI1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\n",
        "y_train = train['Survived'].ravel()\n",
        "train = train.drop(['Survived'], axis=1)\n",
        "x_train = train.values # Creates an array of the train data\n",
        "x_test = test.values # Creats an array of the test data"
      ],
      "metadata": {
        "id": "QdzVvt3YB4aM"
      },
      "id": "QdzVvt3YB4aM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create our OOF train and test predictions. These base results will be used as new features\n",
        "et_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) # Extra Trees\n",
        "rf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) # Random Forest\n",
        "ada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost \n",
        "gb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) # Gradient Boost\n",
        "svc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) # Support Vector Classifier\n",
        "\n",
        "print(\"Training is complete\")"
      ],
      "metadata": {
        "id": "FZIgybuy8pnn"
      },
      "id": "FZIgybuy8pnn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_feature = rf.feature_importances(x_train,y_train)\n",
        "et_feature = et.feature_importances(x_train, y_train)\n",
        "ada_feature = ada.feature_importances(x_train, y_train)\n",
        "gb_feature = gb.feature_importances(x_train,y_train)"
      ],
      "metadata": {
        "id": "-tyG6NDB4-KO"
      },
      "id": "-tyG6NDB4-KO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "KFHVP9o-AC9U"
      },
      "id": "KFHVP9o-AC9U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tw5jIQxNAC5E"
      },
      "id": "tw5jIQxNAC5E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9l_l1edtAC2D"
      },
      "id": "9l_l1edtAC2D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "D2bOy_XeACzr"
      },
      "id": "D2bOy_XeACzr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "KsWw4YQaACxD"
      },
      "id": "KsWw4YQaACxD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7lqXMFIcACue"
      },
      "id": "7lqXMFIcACue",
      "execution_count": null,
      "outputs": []
    }
  ]
}